{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all files in the wiki folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valentin_Yanin.html'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_names = os.listdir(\"wiki\")\n",
    "len(file_names)\n",
    "\n",
    "file_names[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"wiki\", file_names[0])) as f:\n",
    "    file_read = f.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the MapReduce function to this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    pool = Pool(num_processes)\n",
    "    chunk_results = pool.map(mapper, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return functools.reduce(reducer, chunk_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the total number of lines on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_line_count(file_names):\n",
    "    total_lines = 0\n",
    "    for fn in file_names:\n",
    "        with open(os.path.join(\"wiki\", fn)) as f:\n",
    "            total_lines += len(f.readlines())\n",
    "            \n",
    "    return total_lines\n",
    "\n",
    "def reduce_line_count(count1, count2):\n",
    "    return count1 + count2\n",
    "\n",
    "map_reduce(file_names, 8, map_line_count, reduce_line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grep string function\n",
    "\n",
    "We defined a mapreduce_grep_string() function that takes two arguments as input:\n",
    "\n",
    "A path to a folder. In the case of this guided project we will only use it on the wiki folder but having this argument makes the function easier to reuse.\n",
    "\n",
    "The string that we want to find.\n",
    "\n",
    "The mapper function receives a chunk of filenames and calculates all occurrences of the target string on them. If a file contains no occurrences, we chose to not include an entry for that file in the result dictionary.\n",
    "\n",
    "The reducer function uses the dict.update() method to merge the result dictionaries.\n",
    "\n",
    "Note that the target variable will be defined outside and will be the string we are looking for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable is defined outside and contains the string \n",
    "def map_grep(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            if target in line:\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                results[fn].append(line_index)\n",
    "    return results\n",
    "\n",
    "def reduce_grep(lines1, lines2):\n",
    "    lines1.update(lines2)\n",
    "    return lines1\n",
    "\n",
    "def mapreduce_grep(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep, reduce_grep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the occurences of \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"data\"\n",
    "data_occurrences = mapreduce_grep(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allow for case insensitive matches\n",
    "\n",
    "We can allow case insensitive matches by converting both the target and the file contents to lowercase before we match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_insensitive(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            if target.lower() in line:\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                results[fn].append(line_index)\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_insensitive(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_insensitive, reduce_grep)\n",
    "\n",
    "target = \"data\"\n",
    "new_data_occurrences = mapreduce_grep_insensitive(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that we find more matches\n",
    "\n",
    "We already stored the results into variables data_occurrences and new_data_occurrences. To check that we find more matches with the second version of the algorithm, we can loop over the file names and print the length difference between the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 new matches on file wiki/Table_Point_Formation.html\n",
      "Found 1 new matches on file wiki/Ingrid_GuimarC3A3es.html\n",
      "Found 2 new matches on file wiki/Jules_Verne_ATV.html\n",
      "Found 1 new matches on file wiki/Pictogram.html\n",
      "Found 2 new matches on file wiki/Claire_Danes.html\n",
      "Found 1 new matches on file wiki/PTPRS.html\n",
      "Found 1 new matches on file wiki/A_Beautiful_Valley.html\n",
      "Found 1 new matches on file wiki/Mudramothiram.html\n",
      "Found 2 new matches on file wiki/Gordon_Bau.html\n",
      "Found 1 new matches on file wiki/Embraer_Unidade_GaviC3A3o_Peixoto_Airport.html\n",
      "Found 3 new matches on file wiki/Code_page_1023.html\n",
      "Found 1 new matches on file wiki/Cryptographic_primitive.html\n",
      "Found 1 new matches on file wiki/Alex_Kurtzman.html\n",
      "Found 1 new matches on file wiki/Filip_Pyrochta.html\n",
      "Found 1 new matches on file wiki/Morgana_King.html\n",
      "Found 1 new matches on file wiki/Don_Parsons_(ice_hockey).html\n",
      "Found 1 new matches on file wiki/Bias.html\n",
      "Found 2 new matches on file wiki/Tomohiko_ItC58D_(director).html\n",
      "Found 1 new matches on file wiki/Imperial_Venus_(film).html\n",
      "Found 1 new matches on file wiki/Camp_Nelson_Confederate_Cemetery.html\n",
      "Found 1 new matches on file wiki/Benny_Lee.html\n",
      "Found 1 new matches on file wiki/Kul_Gul.html\n",
      "Found 1 new matches on file wiki/Medicago_murex.html\n",
      "Found 1 new matches on file wiki/Oldfield_Baby_Great_Lakes.html\n",
      "Found 1 new matches on file wiki/Wilson_Global_Explorer.html\n",
      "Found 1 new matches on file wiki/Craig_Chester.html\n",
      "Found 1 new matches on file wiki/Derek_Acorah.html\n",
      "Found 1 new matches on file wiki/Jack_Goes_Home.html\n",
      "Found 1 new matches on file wiki/Morning_Glory_(2010_film).html\n",
      "Found 1 new matches on file wiki/Tim_Spencer_(singer).html\n",
      "Found 1 new matches on file wiki/Lower_Blackburn_Grade_Bridge.html\n",
      "Found 1 new matches on file wiki/1953E2809354_FA_Cup_qualifying_rounds.html\n",
      "Found 1 new matches on file wiki/Sol_Eclipse.html\n",
      "Found 1 new matches on file wiki/Jonathan_A._Goldstein.html\n",
      "Found 1 new matches on file wiki/83_(number).html\n",
      "Found 1 new matches on file wiki/Devil_on_Horseback.html\n",
      "Found 1 new matches on file wiki/Harry_Hill_Bandholtz.html\n",
      "Found 2 new matches on file wiki/Shpolskii_matrix.html\n",
      "Found 6 new matches on file wiki/Dragnet_(franchise).html\n",
      "Found 1 new matches on file wiki/Qalat_Kat.html\n",
      "Found 3 new matches on file wiki/Maniitsoq_structure.html\n",
      "Found 1 new matches on file wiki/Ordinary_Virginia.html\n",
      "Found 1 new matches on file wiki/Dewoitine_D.21.html\n",
      "Found 1 new matches on file wiki/Furto_di_sera_bel_colpo_si_spera.html\n",
      "Found 1 new matches on file wiki/Rudy_The_Rudy_Giuliani_Story.html\n",
      "Found 1 new matches on file wiki/Exploratorium_(film).html\n",
      "Found 1 new matches on file wiki/Foulonia.html\n",
      "Found 1 new matches on file wiki/Amborella.html\n",
      "Found 1 new matches on file wiki/Rally_for_Democracy_and_Progress_(Benin).html\n",
      "Found 1 new matches on file wiki/Swathi_Chinukulu.html\n",
      "Found 2 new matches on file wiki/Precorrin6A_reductase.html\n",
      "Found 1 new matches on file wiki/The_Gentleman_Without_a_Residence_(1915_film).html\n",
      "Found 1 new matches on file wiki/Manhattan_Murder_Mystery.html\n",
      "Found 2 new matches on file wiki/Viva_Villa.html\n",
      "Found 1 new matches on file wiki/Companys_procC3A9s_a_Catalunya.html\n",
      "Found 1 new matches on file wiki/Avengers_Academy.html\n",
      "Found 1 new matches on file wiki/Antibiotic_use_in_livestock.html\n",
      "Found 1 new matches on file wiki/Syngenor.html\n",
      "Found 1 new matches on file wiki/Cobble_Hill_Brooklyn.html\n",
      "Found 1 new matches on file wiki/Typhoon_Hester_(1952).html\n",
      "Found 1 new matches on file wiki/WintersWimberley_House.html\n",
      "Found 1 new matches on file wiki/Kokan_Colony.html\n",
      "Found 1 new matches on file wiki/Wilhelm_Wagenfeld_House.html\n",
      "Found 2 new matches on file wiki/Taipa_HousesE28093Museum.html\n",
      "Found 1 new matches on file wiki/WLSR.html\n",
      "Found 1 new matches on file wiki/Lake_County_Examiner.html\n",
      "Found 1 new matches on file wiki/Copamyntis_infusella.html\n",
      "Found 1 new matches on file wiki/C11orf30.html\n",
      "Found 1 new matches on file wiki/Old_Mill_Creek_Illinois.html\n",
      "Found 1 new matches on file wiki/Bahmanabade_Olya.html\n",
      "Found 1 new matches on file wiki/Ek_Dil_Sau_Afsane.html\n",
      "Found 1 new matches on file wiki/Daniel_Cerone.html\n",
      "Found 1 new matches on file wiki/Shoreyjehye_Do.html\n",
      "Found 1 new matches on file wiki/Failing_Office_Building.html\n",
      "Found 1 new matches on file wiki/Pushkar.html\n",
      "Found 1 new matches on file wiki/List_of_Uzbek_films_of_2014.html\n",
      "Found 1 new matches on file wiki/KMTZ.html\n",
      "Found 1 new matches on file wiki/Golabkhvaran.html\n",
      "Found 1 new matches on file wiki/CurtissWright_Hangar_(Columbia_South_Carolina).html\n",
      "Found 1 new matches on file wiki/Blue_SWAT.html\n",
      "Found 1 new matches on file wiki/Danish_Maritime_Safety_Administration.html\n",
      "Found 1 new matches on file wiki/Don_Raye.html\n",
      "Found 1 new matches on file wiki/Lis_LC3B8wert.html\n",
      "Found 1 new matches on file wiki/Doumanaba.html\n",
      "Found 1 new matches on file wiki/Sahanpur.html\n",
      "Found 1 new matches on file wiki/Meleh_Kabude_Sofla.html\n",
      "Found 1 new matches on file wiki/Panchamrutham.html\n",
      "Found 1 new matches on file wiki/Bibiana_Beglau.html\n",
      "Found 1 new matches on file wiki/Kattukukke.html\n",
      "Found 1 new matches on file wiki/Acceptance_(Heroes).html\n",
      "Found 1 new matches on file wiki/Westchester_Los_Angeles.html\n",
      "Found 1 new matches on file wiki/Appa_(film).html\n",
      "Found 1 new matches on file wiki/HD_90156.html\n",
      "Found 2 new matches on file wiki/The_Audacity_to_Podcast.html\n",
      "Found 1 new matches on file wiki/Brownfield_(software_development).html\n",
      "Found 1 new matches on file wiki/Boardman_Township_Mahoning_County_Ohio.html\n",
      "Found 1 new matches on file wiki/King_Parker_House.html\n",
      "Found 2 new matches on file wiki/List_of_Spaghetti_Western_films.html\n",
      "Found 1 new matches on file wiki/The_Future_(film).html\n",
      "Found 1 new matches on file wiki/Weiser_River.html\n",
      "Found 1 new matches on file wiki/Jon_Mullich.html\n",
      "Found 1 new matches on file wiki/Saravan_Gilan.html\n",
      "Found 2 new matches on file wiki/Agaritine_gammaglutamyltransferase.html\n",
      "Found 1 new matches on file wiki/Nuno_Leal_Maia.html\n",
      "Found 1 new matches on file wiki/Battle_of_Wattignies.html\n",
      "Found 1 new matches on file wiki/Colchester_Village_Historic_District.html\n",
      "Found 1 new matches on file wiki/Hayateumi_Hidehito.html\n",
      "Found 7 new matches on file wiki/List_of_people_from_Bangor_Maine.html\n",
      "Found 1 new matches on file wiki/Mirisah.html\n",
      "Found 1 new matches on file wiki/Teiji_Ito.html\n",
      "Found 1 new matches on file wiki/L._Fry.html\n",
      "Found 1 new matches on file wiki/Tropical_sprue.html\n",
      "Found 1 new matches on file wiki/Roxbury_Presbyterian_Church.html\n",
      "Found 1 new matches on file wiki/Peter_Collingwood.html\n",
      "Found 4 new matches on file wiki/List_of_molecular_graphics_systems.html\n",
      "Found 1 new matches on file wiki/Functoid.html\n",
      "Found 1 new matches on file wiki/Vojin_C486etkoviC487.html\n",
      "Found 1 new matches on file wiki/Julien_Boisselier.html\n",
      "Found 1 new matches on file wiki/Jazz_in_Turkey.html\n",
      "Found 2 new matches on file wiki/Kim_Yonghwa.html\n",
      "Found 1 new matches on file wiki/Holly_Golightly_(comics).html\n",
      "Found 1 new matches on file wiki/SalemAuburn_Streets_Historic_District.html\n",
      "Found 2 new matches on file wiki/Kate_Harwood.html\n",
      "Found 1 new matches on file wiki/Gulliver_Mickey.html\n",
      "Found 1 new matches on file wiki/Urs_Burkart.html\n",
      "Found 1 new matches on file wiki/Smilax_laurifolia.html\n",
      "Found 1 new matches on file wiki/Taylor_Williamson.html\n",
      "Found 1 new matches on file wiki/Claudia_Neidig.html\n",
      "Found 1 new matches on file wiki/Dean_Kukan.html\n",
      "Found 1 new matches on file wiki/Demographics_of_American_Samoa.html\n",
      "Found 1 new matches on file wiki/C389cole_des_Mines_de_Douai.html\n",
      "Found 1 new matches on file wiki/Frost_Township_Michigan.html\n",
      "Found 1 new matches on file wiki/Shabbir_Kumar.html\n",
      "Found 1 new matches on file wiki/West_Park_Bridge.html\n"
     ]
    }
   ],
   "source": [
    "for fn in new_data_occurrences:\n",
    "    if fn not in data_occurrences:\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]), fn))\n",
    "    elif len(new_data_occurrences[fn]) > len(data_occurrences[fn]):\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]) - len(data_occurrences[fn]), fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finding all match locations\n",
    "We can use any of the above functions to find all match locations. We will use the third one.\n",
    "\n",
    "After finding all indexes in one line, we need to create pairs by adding the line index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_match_indexes(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            match_indexes = find_match_indexes(line, target.lower())\n",
    "            if fn not in results:\n",
    "                results[fn] = []\n",
    "            results[fn] += [(line_index, match_index) for match_index in match_indexes]\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_match_indexes(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_match_indexes, reduce_grep)\n",
    "\n",
    "target = \"science\"\n",
    "occurrences = mapreduce_grep_match_indexes(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the results\n",
    "Let's display the results. We will create a CSV file listing all occurrences. We will also show the text around each occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# How many character to show before and after the match\n",
    "context_delta = 30\n",
    "\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    rows = [[\"File\", \"Line\", \"Index\", \"Context\"]]\n",
    "    for fn in occurrences:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        for line, index in occurrences[fn]:\n",
    "            start = max(index - context_delta, 0)\n",
    "            end   = index + len(target) + context_delta\n",
    "            rows.append([fn, line, index, lines[line][start:end]])\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "      <th>Index</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>840</td>\n",
       "      <td>embers of the USSR Academy of Sciences\",\"Full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>890</td>\n",
       "      <td>ers of the Russian Academy of Sciences\",\"Demid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>90</td>\n",
       "      <td>href=\"/wiki/Soviet_Academy_of_Sciences\" class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>145</td>\n",
       "      <td>ect\" title=\"Soviet Academy of Sciences\"&gt;Soviet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>173</td>\n",
       "      <td>f Sciences\"&gt;Soviet Academy of Sciences&lt;/a&gt;; he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1440</td>\n",
       "      <td>rs_of_the_USSR_Academy_of_Sciences\" title=\"Cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1502</td>\n",
       "      <td>rs of the USSR Academy of Sciences\"&gt;Full Membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1548</td>\n",
       "      <td>rs of the USSR Academy of Sciences&lt;/a&gt;&lt;/li&gt;&lt;li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1632</td>\n",
       "      <td>of_the_Russian_Academy_of_Sciences\" title=\"Cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1697</td>\n",
       "      <td>of the Russian Academy of Sciences\"&gt;Full Membe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File  Line  Index  \\\n",
       "0  wiki/Valentin_Yanin.html     6    840   \n",
       "1  wiki/Valentin_Yanin.html     6    890   \n",
       "2  wiki/Valentin_Yanin.html    66     90   \n",
       "3  wiki/Valentin_Yanin.html    66    145   \n",
       "4  wiki/Valentin_Yanin.html    66    173   \n",
       "5  wiki/Valentin_Yanin.html   144   1440   \n",
       "6  wiki/Valentin_Yanin.html   144   1502   \n",
       "7  wiki/Valentin_Yanin.html   144   1548   \n",
       "8  wiki/Valentin_Yanin.html   144   1632   \n",
       "9  wiki/Valentin_Yanin.html   144   1697   \n",
       "\n",
       "                                             Context  \n",
       "0  embers of the USSR Academy of Sciences\",\"Full ...  \n",
       "1  ers of the Russian Academy of Sciences\",\"Demid...  \n",
       "2  href=\"/wiki/Soviet_Academy_of_Sciences\" class=...  \n",
       "3  ect\" title=\"Soviet Academy of Sciences\">Soviet...  \n",
       "4  f Sciences\">Soviet Academy of Sciences</a>; he...  \n",
       "5  rs_of_the_USSR_Academy_of_Sciences\" title=\"Cat...  \n",
       "6  rs of the USSR Academy of Sciences\">Full Membe...  \n",
       "7  rs of the USSR Academy of Sciences</a></li><li...  \n",
       "8  of_the_Russian_Academy_of_Sciences\" title=\"Cat...  \n",
       "9  of the Russian Academy of Sciences\">Full Membe...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"results.csv\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
